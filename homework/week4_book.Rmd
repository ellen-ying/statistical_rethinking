---
title: "Week 4 Homework (book)"
author: "Yurun (Ellen) Ying"
date: '2022-06-09'
output: github_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
```

## Problems from the book - Chapter 7

### 7M4

What happen to effective number of parameters when a prior is more concentrated in PSIS and WAIC?

```{r}
# simulate data
X <- rnorm(100, 0, 1)
Y <- rnorm(100, 0.5*X -1, 1)
# fit a model with flat prior
m7m4a <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + b*X,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ), data = data.frame(X = X, Y = Y)
)
# narrower prior
m7m4b <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
   mu <- a + b*X,
    a ~ dnorm(0, 0.1),
    b ~ dnorm(0, 0.1),
    sigma ~ dexp(1)
  ), data = data.frame(X = X, Y = Y)
)

# compare models
compare(m7m4a, m7m4b, func = PSIS)
compare(m7m4a, m7m4b, func = WAIC)

```


When priors get more concentrated, the effective number of parameters decreases.


### 7H1

```{r}
data(Laffer)
d <- Laffer

# fit a linear model
m7h1a <- quap(
  alist(
    tax_revenue ~ dnorm(mu, sigma),
    mu <- a + b*tax_rate,
    a ~ dnorm(0,1),
    b ~ dnorm(0,1),
    sigma ~ dexp(1)
  ), data = d
)

# add a quadratic term
m7h1b <- quap(
  alist(
    tax_revenue ~ dnorm(mu, sigma),
    mu <- a + b1*tax_rate + b2*tax_rate^2,
    a ~ dnorm(0,1),
    c(b1,b2) ~ dnorm(0,1),
    sigma ~ dexp(1)
  ), data = d
)


# inspect the coefficients
precis(m7h1a); precis(m7h1b)
```


```{r, include=FALSE}
plot_tax <- function(model){
  rate_seq <- seq(min(d$tax_rate)-0.5, max(d$tax_rate)+0.5, length.out = 30)
  mu <- link(model, data = list(tax_rate = rate_seq))
  mu_mean <- apply(mu, 2, mean)
  mu_PI <- apply(mu, 2, PI, prob = .97)
  plot(tax_revenue ~ tax_rate, data = d, col = rangi2, 
       xlim = range(d$tax_rate) + c(-0.5, 0.5), ylim = range(d$tax_revenue) + c(-0.5, 0.5),
       xlab = "Tax rate", ylab = "Tax revenue")
  lines(rate_seq, mu_mean)
  shade(mu_PI, rate_seq)
  mtext(paste("Model:", deparse(substitute(model))))
}
```

```{r}
# plot to see the results
plot_tax(m7h1a); plot_tax(m7h1b)

# compare the models
compare(m7h1a, m7h1b, func = WAIC)

```


The model with a polynomial term makes better predictions, but it is only slightly better than the straight-line model. In general, higher tax rate produces higher tax revenue, but very high tax rate seems to reduce the revenue a little.

### 7H2

```{r}
# the PSIS score gives warning
compare(m7h1a, m7h1b, func = PSIS)

# check the pointwise score
set.seed(1999)
laffer_psis <- PSIS(m7h1b, pointwise = TRUE)
laffer_waic <- WAIC(m7h1b, pointwise = TRUE)
plot(laffer_psis$k, laffer_waic$penalty, 
     xlab = "Pareto k", ylab = "WAIC penalty")
points(laffer_psis$k[which(laffer_psis$k > 1)], laffer_waic$penalty[which(laffer_psis$k > 1)],
       col = "red", lwd = 3)
```

There is one influential case. Let's try fitting a model using Student's *t* distribution.

```{r}
m7h2 <- quap(
  alist(
    tax_revenue ~ dstudent(2, mu, sigma),
    mu <- a + b1*tax_rate + b2*tax_rate^2,
    a ~ dnorm(0,1),
    c(b1,b2) ~ dnorm(0,1),
    sigma ~ dexp(1)
  ), data = d
)

# plotting
plot_tax(m7h2)

# see the WAIC and PSIS
laffert_psis <- PSIS(m7h2, pointwise = TRUE)
laffert_waic <- WAIC(m7h2, pointwise = TRUE)
plot(laffert_psis$k, laffert_waic$penalty, xlim = c(-0.15, 1), ylim = c(0, 0.5),
     xlab = "Pareto k", ylab = "WAIC penalty")
```

There is no points with big values of k or pWAIC anymore.


### 7H3

Three islands and five species. Calculate the information entropy for each and use each on to predict the other 2, calculate the KL divergence.

```{r}
bird <- 
  data.frame(
    i1 = rep(0.2, 5),
    i2 = c(0.8, 0.1, 0.05, 0.025, 0.025),
    i3 = c(0.05, 0.15, 0.7, 0.05, 0.05)
  )

# calculate the information entropy for each island
ie <- apply(bird, 2, function (x) -sum(x*log(x)))
ie

# use each island to prediction the other two
kld <- data.frame(
  i1 = rep(NA, 3),
  i2 = rep(NA, 3),
  i3 = rep(NA, 3)
)

for(i in 1:3) {
  kld_i <- apply(bird, 2, 
        function(x) (-sum(x*log(bird[,i]))) - (-sum(x*log(x)))
        )
  kld[i,] <- kld_i
}

kld

```

Island 1 seems to predict the others the best. This may be because the probability for each species is the most "neutral", i.e., each species has an equal probability to occur. When using this model to predict the others, one will surely get a fairly good prediction because it doesn't distinguish between species. And we can see when using island 2 and island 3 to predict island 1, the KL divergence is not very large as well. However, when island 2 and island 3 predict each other, they are fairly far apart.


## Chapter 8

### 8H1 & 8H2

Add a `bed` as a predictor in the tuplis model.

```{r, include=FALSE}
data("tulips")
d <- tulips

# standardization
d$blooms_std <- d$blooms / max(d$blooms) # to preserve the meaningful 0 value
d$water_cent <- d$water - mean(d$water) # centering
d$shade_cent <- d$shade - mean(d$shade)
d$b <-as.integer(d$bed)

# a model with interaction
m8.5 <- quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent,
    a ~ dnorm(0.5, 0.25),
    bw ~ dnorm(0, 0.25),
    bs ~ dnorm(0, 0.25),
    bws ~ dnorm(0, 0.25),
    sigma ~ dexp(1)
  ),
  data = d
)
```

```{r}
# a model including bed as a predictor
m8.6 <- quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a[b] + bw[b]*water_cent + bs[b]*shade_cent + bws[b]*water_cent*shade_cent,
    a[b] ~ dnorm(0.5, 0.25),
    bw[b] ~ dnorm(0, 0.25),
    bs[b] ~ dnorm(0, 0.25),
    bws[b] ~ dnorm(0, 0.25),
    sigma ~ dexp(1)
  ),
  data = d
)

precis(m8.6)

# plotting
par(mfrow = c(3, 3))
for (b in 1:3) {
  
  for (s in -1:1) {
  idx <- which(d$shade_cent == s & d$b == b)
  plot(d$water_cent[idx], d$blooms_std[idx], xlim = c(-1, 1), ylim = c(0, 1),
       xlab = "water", ylab = "blooms", pch = 16, col = rangi2)
  mu <- link(m8.6, data = data.frame(b = b, shade_cent = s, water_cent = -1:1))
  lines(-1:1, apply(mu, 2, mean))
  shade(apply(mu, 2, PI), -1:1)
  }
  
}
```

Compare the WAIC scores of the models

```{r}
compare(m8.5, m8.6, func = WAIC)
```


The comparison shows that the model omitting `bed` makes better predictions than the model including it. The standard error of the difference in WAIC is bigger than the difference itself, indicating that there is uncertainty in whether the difference is reliable. The coefficients of the model of data in different beds are not very different, meaning that there may not be big difference in how water and shade influence blooms in different bed conditions. Therefore, it makes sens that the model without `bed` have better predictive accuracy.

### 8H3

Check the influential cases in the rugged data and use Student-t distribution to fit a new one

```{r, include=FALSE}
data(rugged)
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
d2 <- d[complete.cases(d$rgdppc_2000),]
d2$log_gdp_std <- d2$log_gdp / mean(d2$log_gdp)
d2$rugged_std <- d2$rugged/max(d2$rugged)
m_rugged <- mean(d2$rugged_std)
d2$cid <- ifelse(d2$cont_africa == 1, 1, 2)

m8.3 <- quap(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu <- a[cid] + b[cid] * (rugged_std - m_rugged),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = d2
)
```

```{r}
set.seed(7)
m8.3_waic <- WAIC(m8.3, pointwise = TRUE)
m8.3_psis <- PSIS(m8.3, pointwise = TRUE)

# plot
plot(m8.3_psis$k, m8.3_waic$penalty,
     xlab = "Pareto k", ylab = "WAIC penalty")
abline(v = 0.5, col = "red")

# check which are the influential cases
idx <- which(m8.3_psis$k > 0.5 | m8.3_waic$penalty > 0.5)
inf <- d2[idx,]
inf$country
```

The influential countries do include Seychelles. Let's plot the model against data and see what are happening with these countries.

```{r, include=FALSE}
r_seq <- seq(-0.1, 1.1, length.out = 30)
# African countries
mu_Africa <- link(m8.3, data = data.frame(cid = 1, rugged_std = r_seq))
mu_Africa_mu <- apply(mu_Africa, 2, mean)
mu_Africa_PI <- apply(mu_Africa, 2, PI, prob = .97)

# non-African countries
mu_nonAfrica <- link(m8.3, data = data.frame(cid = 2, rugged_std = r_seq))
mu_nonAfrica_mu <- apply(mu_nonAfrica, 2, mean)
mu_nonAfrica_PI <- apply(mu_nonAfrica, 2, PI, prob = .97)
```

```{r}
par(mfrow = c(1, 2))
plot(NULL, xlim = c(0, 1), ylim = c(0.5, 1.5),
     xlab = "ruggedness", ylab = "logged GDP")
points(d2$rugged_std[d2$cid == 1], d2$log_gdp_std[d2$cid == 1],
       col = rangi2, pch = 16)
points(inf$rugged_std[inf$cid == 1], inf$log_gdp_std[inf$cid == 1], 
       col = "red", lwd = 2)
lines(r_seq, mu_Africa_mu, col = rangi2, lwd = 3)
shade(mu_Africa_PI, r_seq, col = col.alpha(rangi2, 0.2))
mtext("African countries")

plot(NULL, xlim = c(0, 1), ylim = c(0.5, 1.5),
     xlab = "ruggedness", ylab = "logged GDP")
points(d2$rugged_std[d2$cid == 2], d2$log_gdp_std[d2$cid == 2],
       col = "gray", pch = 16)
points(inf$rugged_std[inf$cid == 2], inf$log_gdp_std[inf$cid == 2], 
       col = "red", lwd = 2)
lines(r_seq, mu_nonAfrica_mu, lwd = 3)
shade(mu_nonAfrica_PI, r_seq)
mtext("Non-African countries")
```

Seems that Seychelles has both high ruggedness and high GDP, while Tajikistan has high ruggedness and exceptionally low GDP.

Let's now fit a Student-t distribution to this data.

```{r}
m8.3t <- quap(
  alist(
    log_gdp_std ~ dstudent(2, mu, sigma),
    mu <- a[cid] + b[cid] * (rugged_std - m_rugged),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = d2
)

set.seed(7)
m8.3t_waic <- WAIC(m8.3t, pointwise = TRUE)
m8.3t_psis <- PSIS(m8.3t, pointwise = TRUE)

# plot
plot(m8.3t_psis$k, m8.3t_waic$penalty,
     xlab = "Pareto k", ylab = "WAIC penalty")
abline(v = 0.5, col = "red")
```

There are no extreme outliers in thie model.


### 8H4

Use data `nettle` to evaluate the hypothesis that language diversity is partly a product of food security. In particular, evaluate the main effects of `mean.growing.season` and `sd.growing.season` and their two-way interaction. In all the models, consider log(area) as a covariate.

```{r}
data(nettle)
d <- nettle
# language per capita
d$lang.per.cap <- d$num.lang / d$k.pop
# lang.per.cap and area on a log scale
d$log.lpc <- log(d$lang.per.cap)
d$log.area <- log(d$area)
# standardize mean and sd of growing season on a scale of 0 to 1
d$mu.gs.std <- 
  d$mean.growing.season / (max(d$mean.growing.season) - min(d$mean.growing.season))
d$sd.gs.std <- 
  d$sd.growing.season / (max(d$sd.growing.season) - min(d$sd.growing.season))
```


(a) Evaluate the hypothesis that language diversity, as measured by `log(lang.per.cap)`, is positively associated with the average length of the growing season, `mean.growing.season`. Consider `log(area)` in your regression(s) as a covariate (not an interaction). Interpret your results. 

Prior choices:

- `a` is the log language per capita (lpc) when mean growing season is 0. I choose to use a very weakly informed prior for this - a normal distribution using the mean log lpc as the mean and 1/4 of its range as sd. This allows most possible value of a to lie with the range of the data.
- `bmu` is the increase in log lpc with each unit of increase in mean growing season. The range of log lpc tells us this value is a small positive value (roughly 1/9 to 1/8). I will use dnorm(0, 0.5) as the prior, which allows most values to lie between -1 to 1.

```{r, include=FALSE}
# check the priors
m8h40 <- quap(
  alist(
    log.lpc ~ dnorm(mu, sigma),
    mu <- a + bmu*mu.gs.std,
    a ~ dnorm(-5, 2),
    c(bmu) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

set.seed(1999)
prior <- extract.prior(m8h40)
mu.gs_seq <- seq(0, 1, length.out = 30)
mu <- link(m8h40, post = prior, data = data.frame(mu.gs.std = mu.gs_seq))
plot(NULL, xlab = "Mean growing season (std)", ylab = "Log language per capita",
     xlim = range(d$mu.gs.std), ylim = range(d$log.lpc) + c(-1,1))
abline(h = min(d$log.lpc), lty = 2)
abline(h = max(d$log.lpc), lty = 2)
for (i in 1:50) lines(mu.gs_seq, mu[i,], col = col.alpha("black", 0.3))
```

```{r}
m8h4a<- quap(
  alist(
    log.lpc ~ dnorm(mu, sigma),
    mu <- a + bmu*mu.gs.std + ba*log.area,
    a ~ dnorm(-5, 2),
    c(bmu,ba) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

precis(m8h4a)

# posterior prediction when log area is at its mean
mu.gs_seq <- seq(0, 1, length.out = 30)
mu <- link(m8h4a, data = data.frame(mu.gs.std = mu.gs_seq, log.area = mean(d$log.area)))
plot(log.lpc ~ mu.gs.std, data = d, col = rangi2,  
     xlab = "Mean growing season (std)", ylab = "Log language per capita",
     xlim = range(mu.gs_seq), ylim = range(d$log.lpc) + c(-1,1))
lines(mu.gs_seq, apply(mu, 2, mean))
shade(apply(mu, 2, PI), mu.gs_seq)
```

We can see when the area size is controlled, language diversity as measures by log language number per capita is positively associated with mean length of growing season.


(b) Now evaluate the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, `sd.growing.season`. This hypothesis follows from uncertainty in harvest favoring social insurance through larger social networks and therefore fewer languages. Again, consider `log(area)` as a covariate (not an interaction). Interpret your results. 

```{r}
m8h4b<- quap(
  alist(
    log.lpc ~ dnorm(mu, sigma),
    mu <- a + bsd*sd.gs.std + ba*log.area,
    a ~ dnorm(-5, 2),
    c(bsd,ba) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

precis(m8h4b)

# posterior prediction when log area is at its mean
sd.gs_seq <- seq(0, 1, length.out = 30)
mu <- link(m8h4b, data = data.frame(sd.gs.std = sd.gs_seq, log.area = mean(d$log.area)))
plot(log.lpc ~ sd.gs.std, data = d, col = rangi2,  
     xlab = "SD of growing season (std)", ylab = "Log language per capita",
     xlim = range(sd.gs_seq), ylim = range(d$log.lpc) + c(-1,1))
lines(sd.gs_seq, apply(mu, 2, mean))
shade(apply(mu, 2, PI), sd.gs_seq)
```

When the area size is controlled, language diversity is negatively associated with standard deviation in the length of growing season. However, this effect is very small and is not reliably different from 0.


(c) Finally, evaluate the hypothesis that `mean.growing.season` and `sd.growing.season` interact to synergistically reduce language diversity. The idea is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect windfalls to be used during the droughts.


```{r}
m8h4c<- quap(
  alist(
    log.lpc ~ dnorm(mu, sigma),
    mu <- a + bmu*mu.gs.std + bsd*sd.gs.std + 
      bint*mu.gs.std*sd.gs.std + ba*log.area,
    a ~ dnorm(-5, 2),
    c(bmu,bsd,bint,ba) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

precis(m8h4c)

# draw the triptych graph
# chop the data equally into three parts
# using the mid point of each part to make posterior prediction
mu_seq <- quantile(d$mu.gs.std, prob = seq(0, 1, length.out = 7))
sd_seq <- seq(0, 1, length.out = 30)
par(mfrow = c(1,3))
for (i in 1:3) {
  idx <- which(d$mu.gs.std >= mu_seq[2*i-1] & d$mu.gs.std <= mu_seq[2*i+1])
  mu <- link(m8h4c, data = data.frame(mu.gs.std = unname(mu_seq[2*i]), 
                                      sd.gs.std = sd_seq,
                                      log.area = mean(d$log.area)))
  plot(d$sd.gs.std[idx], d$log.lpc[idx], col = rangi2, pch = 16,
       xlim = c(0,1), ylim = c(-10, 0),
        xlab = "SD of growing season (std)", ylab = "Log language per capita",
       )
  lines(sd_seq, apply(mu, 2, mean))
  shade(apply(mu, 2, PI), sd_seq)
}

```

There doesn't seem to be an interaction effect between average length of growing season and its standard deviation on language diversity as measured by log language per capita.


## Chapter 9

### 9H1

```{r}
mp <- ulam( alist(
  a ~ dnorm(0,1),
  b ~ dcauchy(0,1)
), data=list(y=1) , chains=1 )

precis(mp)

traceplot(mp, n_cols = 2)

post <- extract.samples(mp)

par(mfrow = c(1,2))
dens(post$a)
dens(post$b)
```

This model is directly extracting samples from the two distribution without fitting into any data.

The traceplot for b is a bit wild, it doesn't seem to stay in one distribution, while the one for a looks well-behaved. The distribution of a is approximately normal, while b is not a normal distribution. It peaks at 0 and has some extreme values.


### 9H3 & 9H4

Use the leg data to see how changing the priors of one parameter can unexpectedly influence another, especially when they are highly correlated.

```{r,include=FALSE}
N <- 100
set.seed(909)
height <- rnorm(N, 10, 2) 
leg_prop <- runif(N, 0.4, 0.5) 
leg_left <- leg_prop*height + rnorm(N, 0, 0.02) 
leg_right <- leg_prop*height + rnorm(N, 0, 0.02)
d <- data.frame(height, leg_left, leg_right)
```

```{r}
# fit a model using MCMC
m5.8s <- ulam( 
  alist(
  height ~ dnorm(mu , sigma) ,
  mu <- a + bl*leg_left + br*leg_right , 
  a ~ dnorm(10, 100) ,
  bl ~ dnorm(2, 10) ,
  br ~ dnorm(2, 10) ,
  sigma ~ dexp( 1 )
  ), 
  data = d, chains = 4, log_lik = TRUE,
  start = list(a = 10, bl = 0, br = 0.1, sigma = 1)
)

# change the prior for br so it's strictly positive
m5.8s2 <- ulam( 
  alist(
  height ~ dnorm(mu , sigma) ,
  mu <- a + bl*leg_left + br*leg_right , 
  a ~ dnorm(10, 100) ,
  bl ~ dnorm(2, 10) ,
  br ~ dnorm(2, 10) ,
  sigma ~ dexp( 1 )
  ), 
  data = d, chains = 4, 
  constraints = list(br = "lower=0"), log_lik = TRUE,
  start = list(a = 10, bl = 0, br = 0.1, sigma = 1)
)

plot(coeftab(m5.8s, m5.8s2), par = c("bl", "br"))
```

When br is constrained to be strictly positive, the posterior distribution of `br` has a higher mean value, while `bl` has a lower mean value. Their variance both reduced. This is because the length of both legs is approximately the same variable, therefore in the model, `bl + br` is approximately a constant. When `br` increases, `bl` is sure to decrease.

```{r}
compare(m5.8s, m5.8s2, func = WAIC)
```

The first model has a higer number of effective parameters. This is probably because when the prior of `br` is not constrained, its posterior distribution has higher variance, and so as the sample drawn from it. It increases the variance in each parameter over the posterior distribution, thus increasing the penalty term.


### 9H6

Modify the Metropolis algorithm to write your simple MCMC estimator for globe tossing data and model.

Prior -> knowing how many water tosses we get -> posterior
MCMC is used to get the posterior distribution

(The code sample is only about using an algorithm of sampling to approximate the true distribution, knowing the probability of the current point and the proposed point.)

```{r}
# we observe 6 water from 9 tossings
n_sample <- 1e5
position <- rep(0, n_sample)
p_grid <- seq(0, 1, length.out = 50)
current <- 1
for (i in 1:n_sample){
  
  position[i] <- current
  
  # move left or right
  coin_toss <- ifelse(runif(1) < 0.5, 1, -1)
  
  # the proposal
  proposal <- which(p_grid == current) + coin_toss
  
  # make sure the position move in a ring
  if (proposal > length(p_grid)) proposal <- 1
  if (proposal < 1) proposal <- 50
  
  # the probability on the two end is zero
  # so we always move when we are there
  if (current == 1 | current == 0){
    prob_move <- 1
  } else {
    prob_move <- dbinom(6, 9, prob = p_grid[proposal]) / dbinom(6, size = 9, current)
  }
  
  # move according to the probability
  current <- ifelse(runif(1) < prob_move, p_grid[proposal], current)
}

hist(position, breaks = p_grid,
     xlab = "Probability of water", 
     main = "Posterior distribution of globe tossing")

# the traceplot
plot(1:n_sample, position, type = "l", col = "turquoise", ylim = c(0,1),
     xlab = "", ylab = "")
```


### 9H7

Hamilton Monte Carlo for globe tossing data.

The function of log likelihood: $$\sum_i \log P(x_i|9,p) + \log P(p|0,1)$$, where $x_i \sim Binom(9, p)$ and $p \sim Unif(0,1)$

```{r}
x <- 6
n <- 9

# likelihood function
U <- function(q, min = 0, max = 1) {
  p <- q
  if(q >= 0 & q <= 1)(
    U <- sum(dbinom(x, n, prob = p, log = TRUE)) + dunif(p, min, max, log = TRUE)
  ) else U <- 0
  return(-U)
}
```

The gradient function is:

$$
\begin{aligned}
\frac{dU}{dp} &= \frac{d\sum_i \log P(x_i|9,p)}{dp} \\
&= \frac{d(\sum_i \log \binom{9}{x_i} + \log p^{x_i} + \log (1-p)^{9-x_i})}{dp} \\
&= \sum_i \frac{x_i}{p} + \frac{9-x_i}{1-p}
\end{aligned}
$$

```{r}
# gradient function
U_grad <- function(q, min = 0, max = 1) {
  p <- q
  if(q > 0 & q < 1)(
    G <- sum(x/p + (n-x)/(1-p))
  ) else G <- 0
  return(-G)
}
```

Draw samples. Here epsilon and L need to be tuned.

```{r}
# draw samples
set.seed(1999)
Q <- list()
initial <- 0.2
Q$q <- initial # current position
step <- 0.002 # epsilon
L <- 6 # steps
n_sample <- 1e4
position <- rep(0, n_sample)
accept <- rep(0, n_sample)
  
for (i in 1:n_sample) {
  Q <- HMC2(U, U_grad, step, L, Q$q)
  position[i] <- Q$q
  accept[i] <- Q$accept
}

plot(NULL, xlim = c(0,100), ylim = c(0,1),
       xlab = "", ylab = "")
points(0, initial, pch = 4)

# plot the trace
for (i in 1:100){
  points(i, position[i], 
         pch = ifelse(accept[i] == 1, 1, 16),
         col = ifelse(accept[i] == 1, "black", "red"))
}

# histogram
hist(position[which(accept == 1)], breaks = seq(0, 1, length.out = 30),
     xlab = "Probability of water", 
     main = "Posterior distribution of globe tossing")

# traceplot
plot(1:n_sample, position, ylim = c(0,1), type = "l")
mtext(paste("Number of effective samples:", sum(accept)))
```


